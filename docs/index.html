<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta name="renderer" content="webkit">
        <link rel="icon" sizes="192x192" href="assets/next.jpeg">
        <title>PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance">
        <meta name="author" content="Yang Deng">
        <link href="styles/bootstrap.min.css" rel="stylesheet">
        <link href="styles/common.css" rel="stylesheet">
    </head>

    <body data-spy="scroll" data-target=".navbar">
        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container">
                <!-- Brand and toggle get grouped for better mobile display -->
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <h2 class="navbar-brand hidden-xs hidden-sm">
                        <span class="logo">PACIFIC</span>
                        <span class="description">Proactive Conversational QA in Finance</span>
                    </h2>

                    <h2 class="navbar-brand hidden-md hidden-lg">
                        <span class="logo">PACIFIC</span>
                    </h2>
                </div>

                <div class="collapse navbar-collapse" id="navbar-collapse">
                    <ul class="nav navbar-nav navbar-right">
                        <li><a class="page-scroll" href="#introduction">Introduction</a></li>
                        <li><a class="page-scroll" href="#start">Getting Started</a></li>
                        <li><a class="page-scroll" href="#leaderboard">Leaderboard</a></li>
                        <li><a class="page-scroll" href="#submission">Submission</a></li>
                        <li><a class="page-scroll" href="#contact">Contact</a></li>
                    </ul>
                </div>
            </div>
        </nav>

        <!-- linked this part-->
        <section id="introduction" class="scrollable-section">
            <div class="container">
                <h3>Introduction</h3>
                <p>
                    <b>PACIFIC</b> (<b>P</b>ro<b>A</b>active <b>C</b>onversat<b>I</b>onal question answering in <b>FI</b>nan<b>C</b>e) is a large-scale conversational question answering (CQA) dataset,
                    aiming to stimulate progress of proactive CQA research over more complex and realistic tabular and textual data, especially those requiring numerical reasoning.
                </p>
                <p>The PACIFIC dataset is built from the <a href="https://nextplusplus.github.io/TAT-QA/" target="_blank">TAT-QA dataset</a> by using its question-answer pairs as guidance for constructing conversation sessions. Compared with existing datasets, PACIFIC exhibits three key challenges:</p>

                <ul>
                    <li><b>Proactivity</b>: The system needs to proactively assist the user to clarify their question intent by asking clarifying questions; </li>
                    <li><b>Numerical Reasoning</b>: There are a large number of questions that require numerical reasoning to answer; </li>
                    <li><b>Hybrid Context</b>: The context given is hybrid,  comprising a <ins>semi-structured table </ins> and <ins>at least two relevant paragraphs</ins> that describe, analyze or complement the table. </li>
                </ul>
                <p>
                    In total, PACIFIC contains <b>2,757</b> dialogues associated with <b>19,008</b> QA turns.
               </p>
                <p>
                    The following is an example of PACIFIC and the corresponding example from TAT-QA. The left dashed line box shows a hybrid context.
                </p>


                <img src="assets/pacific-sample.png" alt="PACIFIC Sample" width="100%" >
                    <br>

                For more information, please read our EMNLP 2022 paper  <a href="https://arxiv.org/pdf/2210.08817.pdf" target="_blank">[PDF]</a>.
            </div>
        </section>

        <section id="start" class="scrollable-section">
            <div class="container">
                <h3>Getting Started</h3>

                <ul>
                    <li> <b>Download a copy of the dataset from our</b> <a href="https://github.com/dengyang17/PACIFIC" target="_blank">Github Repo</a>. </li>
                    <li> The data format is nearly identical to TAT-QA, with an additional key for clarification need prediction, i.e., "<b>req_clari</b>" and two additional keys for query rewriting, i.e., "<b>follow_up</b>" and "<b>original_question</b>". </li>
                    <li> To implement the paper method, i.e., UniPCQA, you can follow the instruction in the same Github Repo.  </li>
                    <li> The testing data of PACIFIC contains no answer and is used for evaluation on the leaderboard. </li>
                </ul>

<pre>
{
  "table": {                                                            <em class="comment"># The tabular data in a hybrid context</em>
    "uid": "e78f8b29-6085-43de-b32f-be1a68641be3",                      <em class="comment"># The unique id of a table</em>
    "table": [                                                          <em class="comment"># The table content which is 2d-array</em>
      [
        "",
        "2019",
        "2018",
        "2017"
      ],
      ...
      [
        "Discount rate",
        "2.3",
        "2.5",
        "2.6"
      ]
    ]
  },
  "paragraphs": [                                                        <em class="comment"># The textual data in a hybrid context comprising at least two associated paragraphs to the table</em>
    {
      "uid": "62be4f5a-1693-4e6b-8bb4-0a4e1e40b409",                     <em class="comment"># The unique id of a paragraph</em>
      "order": 1,                                                        <em class="comment"># The order of the paragraph in all associated paragraphs, starting from 1</em>
      "text": "Actuarial assumptions"              <em class="comment"># The content of the paragraph</em>
    },
    ...
  ],
  "questions": [                                                         <em class="comment"># The questions associated to the hybrid context</em>
    {
      "uid": "d0d414be-e75d-43cb-b37a-c96d6367aae2",                     <em class="comment"># The unique id of a question</em>
      "order": 4,                                                        <em class="comment"># The order of the question in the conversation, starting from 1</em>
      "question": "What is the average rate of inflation?",      <em class="comment"># The question itself</em>
      "answer": [                                                   <em class="comment"># The ground-truth answer</em>
      "Which year are you asking about?"
      ],
      "derivation": "",                                       <em class="comment"># The derivation that can be executed to arrive at the ground-truth answer</em>
      "answer_type": "question",                                       <em class="comment"># The answer type including `span`, `spans`, `arithmetic`, `counting`, and `question`.</em>
      "answer_from": "table",                                       <em class="comment"># The source of the answer including `table`, `table` and `table-text`</em>
      "rel_paragraphs": "",                                                <em class="comment"># The orders of the paragraphs that are relied to infer the answer if any.</em>
      "req_comparison": false,                                           <em class="comment"># A flag indicating if `comparison/sorting` is needed to answer the question whose answer is a single span or multiple spans</em>
      "scale": ""                                                 <em class="comment"># The scale of the answer including `None`, `thousand`, `million`, `billion` and `percent`</em>
      "req_clari": true,                                       <em class="comment"># Whether the user question requires clarifications.</em> 
      "follow_up": false,                                       <em class="comment"># Whether the user question is a follow-up question that requires the information from the previous conversation.</em> 
      "original_question": "What is the average rate of inflation?",                                       <em class="comment"># The self-contained question for query rewriting.</em> 
    }
  ]
}
</pre>

            </div>
        </section>

        <section id="leaderboard" class="scrollable-section">
            <div style="background-color: #e8e8e8">
                <div class="container">
                    <h3>Leaderboard</h3>

                    <div class="table-responsive">
                    <table class="table well">
                        <thead class="thead-dark">

                        <tr>
                            <th>Rank</th>
                            <th>Model Name</th>
                            <th>Team Name</th>
                            <th>Exact Match</th>
                            <th>F1</th>
                            <th>Created</th>
                            <th>Paper</th>
                            <th>Codes</th>
                        </tr>
                        </thead>

                        <tbody>
                        <tr>
                            <td>-</td>
                            <td>Human Performance</td>
                            <td>-</td>
                            <td>83.6</td>
                            <td>92.1</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                        </tr>
                        <tr>
                            <td>1</td>
                            <td>HITReasoner</td>
                            <td>Hero_Dirk</td>
                            <td><b>65.8</b></td>
                            <td><b>74.0</b></td>
                            <td>21 Feb 2023</td>
                            <td>N.A.</td>
                            <td>N.A.</td>
                        </tr>
                        
                        <tr>
                            <td>2</td>
                            <td>Baseline - UniPCQA</td>
                            <td>NExT/CUHK</td>
                            <td><b>61.9</b></td>
                            <td><b>70.2</b></td>
                            <td>22 Oct 2022</td>
                            <td><a href="https://arxiv.org/pdf/2210.08817.pdf" target="_blank">Paper</a></td>
                            <td><a href="https://github.com/dengyang17/PACIFIC" target="_blank">Code</a></td>
                        </tr>
                        </tbody>
                    </table>
                    </div>

                </div>
            </div>
        </section>

        <section id="submission" class="scrollable-section">
            <div class="container">
                <h3>Submission</h3>
                <p>
                To evaluate your models, we have also made available the evaluation script we will use for official evaluation,
                To run the evaluation, use
                </p>
                <pre>python pacific_eval.py --gold_path=:path_to_dev --pred_path=:path_to_predictions</pre>

                <h4>Predictions Format</h4>
                <p> The predictions file in JSON format contains a dictionary with question ids as keys and the predictions as values
                    (each prediction shall include both `answer` and `scale` in an array). For example,
                </p>
                <pre>
{
 "9337c3e6-c53f-45a9-836a-02c474ceac16": [
    "4.6",
    "percent"
  ],
  "c4170232-e89c-487a-97c5-afad45e9d702": [
    "16",
    "thousand"
  ],
  "d81d1ae7-363c-4b47-8eea-1906fef33856": [
    ["2018", "2019"],
    ""
  ]
  ...
}
</pre>
                <p>The evaluation and the submission follow the same form as <a href="https://nextplusplus.github.io/TAT-QA/" target="_blank">TAT-QA</a>.</p>

                <h4>Submission</h4>
                <p>
                    Please email the prediction file of the test set with the following information to us:
                    <ul>
                        <li>Model name with a brief description of your model</li>
                        <li>Team name</li>
                        <li>Contact person</li>
                        <li>Email of the contact person</li>
                        <li>Link to the paper (if any)</li>
                        <li>Link to the code repository (if any)</li>
                    </ul>
                </p>
                <p>
                Please give us up to two weeks to evaluate your submission and we will add your model to the leaderboard.
                </p>
            </div>
        </section>

        <section id="contact" class="scrollable-section">
            <div class="container">
                <h3>Contact</h3>
                <p>
                    For more information, please contact:
                    <ul>
                        <li>Yang DENG <a href="mailto:dengyang17dydy@gmail.com">dengyang17dydy@gmail.com</a></li>
                    </ul>
                </p>
                <p>
                    Please kindly cite our work if you use our dataset or codes, thank you.
                </p>
<pre>

    @inproceedings{emnlp22pacific,
        author    = {Yang Deng and
                     Wenqiang Lei and
                     Wenxuan Zhang and
                     Wai Lam and
                     Tat{-}Seng Chua},
        title     = {{PACIFIC:} Towards Proactive Conversational Question Answering over
                     Tabular and Textual Data in Finance},
        booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural
                     Language Processing, {EMNLP} 2022},
        pages     = {6970--6984},
        year      = {2022},
      }

</pre>
            </div>
            </div>
        </section>

        <footer class="footer" style="padding-top: 50px;">
            <div class="container">
                <hr>
                <p style="font-style: italic; text-align: center">
                    Copyright &copy; 2018-2021 NExT++ /
                    <a class="black" href="http://www.nextcenter.org/privacy-policy" target="_blank" >Privacy Policy</a> /
                    <a class="black" href="http://www.nextcenter.org/terms-conditions" target="_blank" >Terms &amp; Conditions</a>
                </p>
            </div>
        </footer>

        <script type="text/javascript" src="scripts/jquery-3.2.1.min.js"></script>
        <script type="text/javascript" src="scripts/bootstrap.min.js"></script>
        <script type="text/javascript" src="scripts/jquery.easing.min.js"></script>
        <script type="text/javascript" src="scripts/scrolling-nav.js"></script>
    </body>
</html>
